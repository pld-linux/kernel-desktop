diff -Nur linux-2.6.17/arch/i386/kernel/cpu/proc.c linux-2.6.17.cpuset_virt/arch/i386/kernel/cpu/proc.c
--- linux-2.6.17/arch/i386/kernel/cpu/proc.c	2006-06-18 01:49:35.000000000 +0000
+++ linux-2.6.17.cpuset_virt/arch/i386/kernel/cpu/proc.c	2006-06-19 13:51:54.000000000 +0000
@@ -4,6 +4,7 @@
 #include <asm/semaphore.h>
 #include <linux/seq_file.h>
 #include <linux/cpufreq.h>
+#include <linux/cpuset.h>
 
 /*
  *	Get CPU information for use by the procfs.
@@ -80,12 +81,20 @@
 	if (!cpu_online(n))
 		return 0;
 #endif
+#ifdef CONFIG_CPUSETS
+	if (!cpu_visible_in_cpuset(n, current->cpuset))
+		return 0;
+#endif
 	seq_printf(m, "processor\t: %d\n"
 		"vendor_id\t: %s\n"
 		"cpu family\t: %d\n"
 		"model\t\t: %d\n"
 		"model name\t: %s\n",
+#ifdef CONFIG_CPUSETS
+		cpuid_in_cpuset(n, current->cpuset),
+#else
 		n,
+#endif
 		c->x86_vendor_id[0] ? c->x86_vendor_id : "unknown",
 		c->x86,
 		c->x86_model,
diff -Nur linux-2.6.17/arch/ia64/kernel/setup.c linux-2.6.17.cpuset_virt/arch/ia64/kernel/setup.c
--- linux-2.6.17/arch/ia64/kernel/setup.c	2006-06-18 01:49:35.000000000 +0000
+++ linux-2.6.17.cpuset_virt/arch/ia64/kernel/setup.c	2006-06-19 13:51:54.000000000 +0000
@@ -44,6 +44,7 @@
 #include <linux/initrd.h>
 #include <linux/pm.h>
 #include <linux/cpufreq.h>
+#include <linux/cpuset.h>
 
 #include <asm/ia32.h>
 #include <asm/machvec.h>
@@ -515,6 +516,11 @@
 	unsigned long proc_freq;
 	int i;
 
+#ifdef CONFIG_CPUSETS
+	if (!cpu_visible_in_cpuset(cpunum, current->cpuset))
+		return 0;
+#endif
+	
 	mask = c->features;
 
 	switch (c->family) {
@@ -563,7 +569,12 @@
 		   "cpu MHz    : %lu.%06lu\n"
 		   "itc MHz    : %lu.%06lu\n"
 		   "BogoMIPS   : %lu.%02lu\n",
-		   cpunum, c->vendor, family, c->model, c->revision, c->archrev,
+#ifdef CONFIG_CPUSETS
+		   cpuid_in_cpuset(cpunum, current->cpuset),
+#else
+		   cpunum,
+#endif
+		   c->vendor, family, c->model, c->revision, c->archrev,
 		   features, c->ppn, c->number,
 		   proc_freq / 1000, proc_freq % 1000,
 		   c->itc_freq / 1000000, c->itc_freq % 1000000,
diff -Nur linux-2.6.17/arch/x86_64/kernel/setup.c linux-2.6.17.cpuset_virt/arch/x86_64/kernel/setup.c
--- linux-2.6.17/arch/x86_64/kernel/setup.c	2006-06-18 01:49:35.000000000 +0000
+++ linux-2.6.17.cpuset_virt/arch/x86_64/kernel/setup.c	2006-06-19 13:52:23.000000000 +0000
@@ -46,6 +46,7 @@
 #include <linux/cpufreq.h>
 #include <linux/dmi.h>
 #include <linux/dma-mapping.h>
+#include <linux/cpuset.h>
 #include <linux/ctype.h>
 
 #include <asm/mtrr.h>
@@ -1332,13 +1333,20 @@
 	if (!cpu_online(c-cpu_data))
 		return 0;
 #endif
-
+#ifdef CONFIG_CPUSETS
+	if (!cpu_visible_in_cpuset(c-cpu_data, current->cpuset))
+		return 0;
+#endif
 	seq_printf(m,"processor\t: %u\n"
 		     "vendor_id\t: %s\n"
 		     "cpu family\t: %d\n"
 		     "model\t\t: %d\n"
 		     "model name\t: %s\n",
+#ifdef CONFIG_CPUSETS
+		     cpuid_in_cpuset(c-cpu_data, current->cpuset),
+#else
 		     (unsigned)(c-cpu_data),
+#endif
 		     c->x86_vendor_id[0] ? c->x86_vendor_id : "unknown",
 		     c->x86,
 		     (int)c->x86_model,
diff -Nur linux-2.6.17/fs/proc/proc_misc.c linux-2.6.17.cpuset_virt/fs/proc/proc_misc.c
--- linux-2.6.17/fs/proc/proc_misc.c	2006-06-19 13:41:22.000000000 +0000
+++ linux-2.6.17.cpuset_virt/fs/proc/proc_misc.c	2006-06-19 13:54:23.000000000 +0000
@@ -45,6 +45,7 @@
 #include <linux/jiffies.h>
 #include <linux/sysrq.h>
 #include <linux/vmalloc.h>
+#include <linux/cpuset.h>
 #include <linux/crash_dump.h>
 #include <asm/uaccess.h>
 #include <asm/pgtable.h>
@@ -468,6 +469,10 @@
 	for_each_possible_cpu(i) {
 		int j;
 
+#ifdef CONFIG_CPUSETS
+		if (!cpu_visible_in_cpuset(i, current->cpuset))
+			continue;
+#endif
 		user = cputime64_add(user, kstat_cpu(i).cpustat.user);
 		nice = cputime64_add(nice, kstat_cpu(i).cpustat.nice);
 		system = cputime64_add(system, kstat_cpu(i).cpustat.system);
@@ -499,6 +504,10 @@
 
 	for_each_online_cpu(i) {
 
+#ifdef CONFIG_CPUSETS
+		if (!cpu_visible_in_cpuset(i, current->cpuset))
+			continue;
+#endif
 		/* Copy values here to work around gcc-2.95.3, gcc-2.96 */
 		user_rt = kstat_cpu(i).cpustat.user_rt;
 		system_rt = kstat_cpu(i).cpustat.system_rt;
@@ -511,7 +520,11 @@
 		softirq = kstat_cpu(i).cpustat.softirq;
 		steal = kstat_cpu(i).cpustat.steal;
 		seq_printf(p, "cpu%d %llu %llu %llu %llu %llu %llu %llu %llu %llu %llu\n",
+#ifdef CONFIG_CPUSETS
+			cpuid_in_cpuset(i, current->cpuset),
+#else
 			i,
+#endif
 			(unsigned long long)cputime64_to_clock_t(user),
 			(unsigned long long)cputime64_to_clock_t(nice),
 			(unsigned long long)cputime64_to_clock_t(system),
diff -Nur linux-2.6.17/include/linux/cpuset.h linux-2.6.17.cpuset_virt/include/linux/cpuset.h
--- linux-2.6.17/include/linux/cpuset.h	2006-06-18 01:49:35.000000000 +0000
+++ linux-2.6.17.cpuset_virt/include/linux/cpuset.h	2006-06-19 13:51:54.000000000 +0000
@@ -63,6 +63,9 @@
 	return current->flags & PF_SPREAD_SLAB;
 }
 
+int cpu_visible_in_cpuset(int cpu, struct cpuset * cs);
+int cpuid_in_cpuset(int cpu, struct cpuset * cs);
+
 #else /* !CONFIG_CPUSETS */
 
 static inline int cpuset_init_early(void) { return 0; }
diff -Nur linux-2.6.17/include/linux/init_task.h linux-2.6.17.cpuset_virt/include/linux/init_task.h
--- linux-2.6.17/include/linux/init_task.h	2006-06-19 13:41:23.000000000 +0000
+++ linux-2.6.17.cpuset_virt/include/linux/init_task.h	2006-06-19 13:51:54.000000000 +0000
@@ -91,6 +91,7 @@
 	.normal_prio	= MAX_PRIO-20,					\
 	.policy		= SCHED_NORMAL,					\
 	.cpus_allowed	= CPU_MASK_ALL,					\
+	.cpus_virt_allowed	= CPU_MASK_ALL,				\
 	.mm		= NULL,						\
 	.active_mm	= &init_mm,					\
 	.run_list	= LIST_HEAD_INIT(tsk.run_list),			\
diff -Nur linux-2.6.17/include/linux/sched.h linux-2.6.17.cpuset_virt/include/linux/sched.h
--- linux-2.6.17/include/linux/sched.h	2006-06-19 13:41:26.000000000 +0000
+++ linux-2.6.17.cpuset_virt/include/linux/sched.h	2006-06-19 13:51:54.000000000 +0000
@@ -1062,6 +1062,7 @@
   	struct mempolicy *mempolicy;
 	short il_next;
 #endif
+	cpumask_t cpus_virt_allowed;
 #ifdef CONFIG_CPUSETS
 	struct cpuset *cpuset;
 	nodemask_t mems_allowed;
diff -Nur linux-2.6.17/kernel/cpuset.c linux-2.6.17.cpuset_virt/kernel/cpuset.c
--- linux-2.6.17/kernel/cpuset.c	2006-06-18 01:49:35.000000000 +0000
+++ linux-2.6.17.cpuset_virt/kernel/cpuset.c	2006-06-19 13:51:54.000000000 +0000
@@ -106,6 +106,7 @@
 	CS_CPU_EXCLUSIVE,
 	CS_MEM_EXCLUSIVE,
 	CS_MEMORY_MIGRATE,
+	CS_VIRTUALIZED,
 	CS_REMOVED,
 	CS_NOTIFY_ON_RELEASE,
 	CS_SPREAD_PAGE,
@@ -123,6 +124,10 @@
 	return test_bit(CS_MEM_EXCLUSIVE, &cs->flags);
 }
 
+static inline int is_virtualized(const struct cpuset *cs)
+{
+	return !!test_bit(CS_VIRTUALIZED, &cs->flags);
+}
 static inline int is_removed(const struct cpuset *cs)
 {
 	return test_bit(CS_REMOVED, &cs->flags);
@@ -699,6 +704,145 @@
 		is_mem_exclusive(p) <= is_mem_exclusive(q);
 }
 
+#define cyclic_next_cpu(index, mask)	__cyclic_next_cpu(index, &mask)
+static inline int __cyclic_next_cpu(int index, const cpumask_t * mask)
+{
+	int i;
+	i = next_cpu(index, *mask);
+	if (i >= NR_CPUS) {
+		if (cpu_isset(0, *mask))
+			return 0;
+		i = next_cpu(0, *mask);
+	}
+	return i;
+}
+
+/**
+ *	cpuset_combine_mask - translate a user cpu mask to a physical one.
+ *	@virt_allowed:	the mask given by the user to sched_setaffinity()
+ *	@cs_allowed:	the mask of the current cpuset.
+ *
+ *	Returns combined mask in *mask.
+ */
+static int combine_mask(cpumask_t *mask, const cpumask_t virt_allowed, const cpumask_t cs_allowed)
+{
+	int i;
+
+	/* start with current cpu out of the mask
+	 * so the first call to next_cpu will take the first cpu
+	 * even if it is cpu zero
+	 */
+	int cpu = NR_CPUS;
+	cpus_clear(*mask);
+
+	if (cpus_empty(virt_allowed)) return 0;
+	if (cpus_empty(cs_allowed)) return 0;
+
+	for (i = 0; i < NR_CPUS; i++) {
+		cpu = cyclic_next_cpu(cpu, cs_allowed);
+		if (cpu_isset(i, virt_allowed))
+			cpu_set(cpu, *mask);
+	}
+	return 0;
+}
+
+/**
+ * Find out whether a cpu should be listed in /proc/cpuinfo
+ *
+ * For virtualized cpusets, only cpus present in the cpuset are shown
+ */
+int cpu_visible_in_cpuset(int cpu, struct cpuset * cs)
+{
+	/* all cpus are visible in non-virtualized cpusets */
+	if (!is_virtualized(cs))
+		return 1;
+
+	return cpu_isset(cpu, cs->cpus_allowed);
+}
+
+/**
+ *	cpuid_in_cpuset - translate a "real" cpu number to a "inside cpuset" (logical)
+ *	@cs:	the cpuset where all the magic occurs.
+ *	@cpu:	cpu number to be translated
+ *
+ *	Used for /proc/cpuinfo.
+ *	Returns the translated cpu number.
+ */
+int cpuid_in_cpuset(int cpu, struct cpuset * cs)
+{
+	int i;
+	int l = 0;
+	
+	/* translation needed only for virtualized cpusets */
+	if (!is_virtualized(cs))
+		return cpu;
+		
+	for(i=0; i < NR_CPUS; i++)
+	{
+		if (i == cpu) return l;
+		if (cpu_isset(i, cs->cpus_allowed))
+			l++;
+	}
+	/* NOT REACHED */
+	BUG();
+	return 0;
+}
+
+/**
+ *	set_cpus_virt_allowed - updated cpus_virt_allowed AND cpus_allowed masks
+ *	@virt_allowed:        the mask given by the user to sched_setaffinity()
+ *	@p:		the task
+ *
+ *	This function does not mess with scheduler internals. Here we rely
+ *	on set_cpus_allowed(), that should, for instance, migrate the task 
+ *	if necessary.
+ */
+static int set_cpus_virt_allowed(task_t *p, cpumask_t mask)
+{
+	cpumask_t new_mask;
+	int retval;
+
+	p->cpus_virt_allowed = mask;
+	combine_mask(&new_mask, p->cpus_virt_allowed, p->cpuset->cpus_allowed);
+	retval = set_cpus_allowed(p, new_mask);
+	return retval;
+}
+
+/**
+ *	This is the exported entry point that will be called
+ *	by sched_setaffinity().
+ */
+int cpuset_set_cpus_affinity(task_t *p, cpumask_t mask)
+{
+	int retval;
+
+	down(&callback_sem);
+	if (is_virtualized(p->cpuset))
+		retval = set_cpus_virt_allowed(p, mask);
+	else {
+		cpumask_t cpus_allowed;
+		cpus_allowed = cpuset_cpus_allowed(p);
+		cpus_and(mask, mask, cpus_allowed);
+		retval = set_cpus_allowed(p, mask);
+	}
+	up(&callback_sem);
+	return retval;
+}
+
+/**
+ *	This is the exported entry point that will be called
+ *	by sched_getaffinity().
+ */
+int cpuset_get_cpus_virt_affinity(task_t *p, cpumask_t *mask)
+{
+	if (is_virtualized(p->cpuset)) {
+		*mask = p->cpus_virt_allowed;
+		return 0;
+	}
+	return -1;
+}
+
+
 /*
  * validate_change() - Used to validate that any proposed cpuset change
  *		       follows the structural rules for cpusets.
@@ -733,6 +877,11 @@
 	if ((par = cur->parent) == NULL)
 		return 0;
 
+	/* virtualization can only be turned on/off on empty cpusets  */
+	if ((atomic_read(&cur->count) > 0) || (!list_empty(&cur->children)))
+		if (is_virtualized(cur) != is_virtualized(trial))
+			return -EBUSY;
+
 	/* We must be a subset of our parent cpuset */
 	if (!is_cpuset_subset(trial, par))
 		return -EACCES;
@@ -1216,11 +1365,29 @@
 		return -ESRCH;
 	}
 	atomic_inc(&cs->count);
+
+	/* depending on current and future cpuset for this task,
+	 * affinity masks may be meaningful or not
+	 */
+	cpumask_t virt_allowed, allowed;
+	if (is_virtualized(cs) == is_virtualized(tsk->cpuset)) {
+		virt_allowed = tsk->cpus_virt_allowed;
+		allowed = tsk->cpus_allowed;
+	} else {
+		virt_allowed = CPU_MASK_ALL;
+		allowed = CPU_MASK_ALL;
+	}
+		
 	rcu_assign_pointer(tsk->cpuset, cs);
 	task_unlock(tsk);
 
-	guarantee_online_cpus(cs, &cpus);
-	set_cpus_allowed(tsk, cpus);
+
+	if (is_virtualized(cs))
+		set_cpus_virt_allowed(tsk, virt_allowed);
+	else {
+		guarantee_online_cpus(cs, &cpus);
+		set_cpus_allowed(tsk, cpus);
+	}
 
 	from = oldcs->mems_allowed;
 	to = cs->mems_allowed;
@@ -1252,6 +1419,7 @@
 	FILE_MEMLIST,
 	FILE_CPU_EXCLUSIVE,
 	FILE_MEM_EXCLUSIVE,
+	FILE_VIRTUALIZE,
 	FILE_NOTIFY_ON_RELEASE,
 	FILE_MEMORY_PRESSURE_ENABLED,
 	FILE_MEMORY_PRESSURE,
@@ -1304,6 +1472,9 @@
 	case FILE_MEM_EXCLUSIVE:
 		retval = update_flag(CS_MEM_EXCLUSIVE, cs, buffer);
 		break;
+	case FILE_VIRTUALIZE:
+		retval = update_flag(CS_VIRTUALIZED, cs, buffer);
+		break;
 	case FILE_NOTIFY_ON_RELEASE:
 		retval = update_flag(CS_NOTIFY_ON_RELEASE, cs, buffer);
 		break;
@@ -1421,6 +1592,9 @@
 	case FILE_MEM_EXCLUSIVE:
 		*s++ = is_mem_exclusive(cs) ? '1' : '0';
 		break;
+	case FILE_VIRTUALIZE:
+		*s++ = is_virtualized(cs) ? '1' : '0';
+		break;
 	case FILE_NOTIFY_ON_RELEASE:
 		*s++ = notify_on_release(cs) ? '1' : '0';
 		break;
@@ -1782,6 +1956,11 @@
 	.private = FILE_MEM_EXCLUSIVE,
 };
 
+static struct cftype cft_virtualize = {
+	.name = "virtualize",
+	.private = FILE_VIRTUALIZE,
+};
+
 static struct cftype cft_notify_on_release = {
 	.name = "notify_on_release",
 	.private = FILE_NOTIFY_ON_RELEASE,
@@ -1824,6 +2003,8 @@
 		return err;
 	if ((err = cpuset_add_file(cs_dentry, &cft_mem_exclusive)) < 0)
 		return err;
+	if ((err = cpuset_add_file(cs_dentry, &cft_virtualize)) < 0)
+		return err;
 	if ((err = cpuset_add_file(cs_dentry, &cft_notify_on_release)) < 0)
 		return err;
 	if ((err = cpuset_add_file(cs_dentry, &cft_memory_migrate)) < 0)
diff -Nur linux-2.6.17/kernel/kthread.c linux-2.6.17.cpuset_virt/kernel/kthread.c
--- linux-2.6.17/kernel/kthread.c	2006-06-19 13:41:25.000000000 +0000
+++ linux-2.6.17.cpuset_virt/kernel/kthread.c	2006-06-19 13:51:54.000000000 +0000
@@ -241,6 +241,15 @@
 	wait_task_inactive(k);
 	set_task_cpu(k, cpu);
 	k->cpus_allowed = cpumask_of_cpu(cpu);
+#ifdef CONFIG_CPUSETS
+	/* kthreads don't use sched_setaffinity() to bind themselves to
+	 * CPUs, we need to take care.
+	 * This should not be problem since it is unlikely that kthreads
+	 * will run in a virtualized cpuset.
+	 * But better be ready, so:
+	 */
+	k->cpus_virt_allowed = cpumask_of_cpu(cpu);
+#endif
 }
 EXPORT_SYMBOL(kthread_bind);
 
diff -Nur linux-2.6.17/kernel/sched.c linux-2.6.17.cpuset_virt/kernel/sched.c
--- linux-2.6.17/kernel/sched.c	2006-06-19 13:41:25.000000000 +0000
+++ linux-2.6.17.cpuset_virt/kernel/sched.c	2006-06-19 13:51:54.000000000 +0000
@@ -4539,11 +4539,15 @@
 	return retval;
 }
 
+#ifdef CONFIG_CPUSETS
+int cpuset_set_cpus_affinity(task_t *p, cpumask_t mask);
+int cpuset_get_cpus_virt_affinity(task_t *p, cpumask_t *mask);
+#endif
+
 long sched_setaffinity(pid_t pid, cpumask_t new_mask)
 {
 	task_t *p;
 	int retval;
-	cpumask_t cpus_allowed;
 
 	lock_cpu_hotplug();
 	read_lock(&tasklist_lock);
@@ -4568,9 +4572,11 @@
 			!capable(CAP_SYS_NICE))
 		goto out_unlock;
 
-	cpus_allowed = cpuset_cpus_allowed(p);
-	cpus_and(new_mask, new_mask, cpus_allowed);
-	retval = set_cpus_allowed(p, new_mask);
+#ifdef CONFIG_CPUSETS
+	retval = cpuset_set_cpus_affinity(p, new_mask);
+#else
+  	retval = set_cpus_allowed(p, new_mask);
+#endif
 
 out_unlock:
 	put_task_struct(p);
@@ -4637,7 +4643,12 @@
 		goto out_unlock;
 
 	retval = 0;
-	cpus_and(*mask, p->cpus_allowed, cpu_online_map);
+#ifdef CONFIG_CPUSETS
+	if (cpuset_get_cpus_virt_affinity(p, mask) < 0)
+		cpus_and(*mask, p->cpus_allowed, cpu_online_map);
+#else
+  	cpus_and(*mask, p->cpus_allowed, cpu_online_map);
+#endif
 
 out_unlock:
 	read_unlock(&tasklist_lock);
